---
title: "Twisted Bootstrap"
author: "Nathan Morris, Will Landau"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tboot}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Twisted Bootstrap

The twisted bootstrap is a weighted resampling technique. The goal is to take a reasonably realistic sample of the rows in such a way that the column means approximate a user-defined target. In other words, you can control the marginal means while still preserving intricate relationships among the variables.

# Example

```{r setup, cache = TRUE}
library(tboot)
set.seed(2017)
```

Simulate a dataset.

```{r sim, cache = TRUE}
dataset <- dichotomous(nrow = 200)
```

The variables are dichotomous.

```{r dataset, cache = TRUE}
head(dataset)
colMeans(dataset)
```

In the bootstrapped dataset, we want all the column means to be around 0.4.

```{r target, cache = TRUE}
target <- rep(0.4, ncol(dataset))
```

Get the row-level resampling weights. The weights are as close to uniform as possible while still making sure the bootstrap approximates the target column means.

```{r weights, cache = TRUE}
weights <- tweights(dataset = dataset, target = target)
```

Bootstrap the rows of the dataset using the weights.

```{r boot, cache = TRUE}
boot <- tboot(dataset = dataset, weights = weights, nrow = 1e5)
```

The column means are close to the target even though all the rows came from the original data.

```{r compare, cache = TRUE}
colMeans(boot)
```

The weights for each sample will now differ:
```{r hist, cache = TRUE}
hist(weights, breaks=25)
abline(v=1/200,col="red")
```

The red line in the histogram above represents the probability for uniform resampling. We recomend against using the resampling methods implemented in this package when the weight of any one sample is too high. 


# Methodology

The goal is to find a bootstrap resampling distribution that

1. is close to uniform. That way, the twisted bootstrap is as close to an ordinary bootstrap as possible.
2. generates bootstrap datasets with the desired column means.


## Stating the optimization problem

Let $q = (q_1, \ldots, q_n)$ be the resampling distribution. Here, $n$ is the number of rows in the original dataset, and $q_i$ is the probability that row $i$ will be chosen for a given row of the bootstrap dataset. Let $p = (p_1, \ldots, p_n)$ be uniform (i.e., $p_i = 1/n$, the resampling distribution of the ordinary bootstrap). 

We want to find a $q$ that is as close to $p$ as possible. We consider two possible measures of distance. First, the Euclidian distance is defined as:
$$
D_{Euclidian}= \sum_{i = 1}^n (p_i - q_i)^2
$$
Second, the Kullback-Leibler (KL) divergence (wich is technically not considered a distance metric) from $p$ to $q$ is defined as: 

$$
\begin{aligned}
D_{KL}(p||q) 
&= \sum_{i = 1}^n p_i \log \frac{p_i}{q_i} \\
&=  \sum_{i = 1}^n \frac{1}{n} \log \frac{1}{n} - \frac{1}{n} \sum_{i = 1}^n \log q_i
\end{aligned}
$$

Note that minimizing $D_{KL}(p||q)$ is the same as minimizing $-\sum_{i = 1}^n \log q_i$.

Thus the goal may be formulated as a constrained optimization proplem:
$$
q=\underset{q \in S}{\operatorname{argmin}} {D(p,q)}
$$
where,

$$
S=\{q|\space q_i>0 \space for \space i \in {1...n}, \space and \space  \sum_{i = 1}^n q_i=1, \space and \space X^Tq = y \space \}.
$$
Here $y$ is a vector of target means for the columns.

## Solving the optimization problem
In the case of the Euclidian distance the problem is a standard "Quadratic Programing" problem which we solve using the 'solve.QP' function from the 'quadprog' package.

To solve the problem for the KL distance, we use the lagrangian approach. Define $A=[X|1]$ (i.e. $A$ is the concatination of $X$ with a conformal column of all ones). Also, let $b=[y^T|1]^T$ (i.e $b$ is the $y$ vector with a one element concatinated to the end). The Lagrangian for this optimization problem is

$$
\begin{aligned}
L(q, \lambda) &= -\sum_{i = 1}^n \log q_i + \lambda (A'q - b) \\
&= \sum_{i = 1}^n \left [ -\log q_i +  \sum_{k = 1}^{K+1} \lambda_k (A_{ik} q_i - b_k) \right ]
\end{aligned}
$$

where 

- The vector $\lambda$ of length $K+1$ represents the Lagrange multipliers.
- $K$ is the number of columns of $X$.
- $A_{ik}$ is the element of $A$ in row $i$ and column $k$.


To optimize, we start by setting the gradient of $L$ equal to zero and solve for a locally optimal $q$ in terms of $\lambda$.

$$
\begin{aligned}
0 &= \left . \frac{\partial L}{\partial q_i} \right |_{(q, \ \lambda)}  \\
&= -\frac{1}{q_i} + A_i' \lambda \\
\frac{1}{q_i} &=   A_i' \lambda \\
q_i &= \frac{1}{  A_i' \lambda}
\end{aligned}
$$

Here, $X_i$ is the $i$'th row of $X$ (expressed as a column vector).  Hence, the locally optimal $q$ in terms of $\lambda$ is 

$$q^*(\lambda) = \left (\frac{1}{A_1^T \lambda}, \ldots, \frac{1}{  A_n^T \lambda} \right ) $$


To get the best $q^*(\lambda)$, we choose the $\lambda^*$ sucth that $$F(\lambda):=A^Tq^*(\lambda) - b=0.$$

We solve this problem by using a modification of Newton's method. Let $F'(\lambda)$ be the first derivative of $F(\lambda)$. We use the starting value of $\lambda^{(1)}=[0,..,0,1]$. At step $n+1$ the proposed value based on Newtons Method would be: $$\lambda^{(n+1)}_*=\lambda^{(n)}-[F'(\lambda^{(n)})]^{-1}F(\lambda^{(n)}).$$
if $q^*(\lambda^{(n+1)}_*)$ is positive for all elements then we set $\lambda^{(n+1)}=\lambda^{(n+1)}_*$. Otherwise, we set  $\lambda^{(n+1)}=\alpha\lambda^{(n+1)}_*+ (1-\alpha)\lambda^{(n)}$ where alpha is set so that the step is half way to producing a negative probability.




# Problems encountered for some data
## Not full rank
If the data matrix is not full rank tweights will fail. Consider reducing the number of columns being constrained to deal with this issue.

## Constraint not achievable
Sometimes the desired constraint may not be achievable. If this is the case, tweights will give a warning, then it will find the "closest" achievable constraint. "Closest" is measured by a weighted euclidian distance. Thus the target ($y$) is replaced with:
$$
y_{new}=X'q^*
$$
where
$$
q^*=\underset{q^* \in S^*}{\operatorname{argmin}} {(X'q - y)'W(X'q - y)}
$$
and $S^*={q^*|\space \sum_{i = 1}^n q_i^* = 1}$. The weights ($W$) is a diagonal matrix with diagonal entries $W_{kk}=1/s^2_k$ where $s^2_k$ is the sample variance of column $k$ from the data matrix ($X$). We solve this problem using the 'ipop' function in the 'kernlap' package. Note that 'kernlab' is used instead of 'quadprog' becuase the problem is not easily formulated using a full rank matrix for the quadratic form in the problem.

## Unable to find an optimum
For the KL dististance, the algorythm may fail to converge properly, and an error or warning will be issued. 

