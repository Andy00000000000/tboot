---
title: "Some testing cases of the `tboot` package"
author: "Lanfeng Pan"
date: "`r Sys.Date()`"
bibliography: bibliography.bib
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE
)
```

# Introduction
The idea of bootstrap [@efron1981nonparametric] is to obtain samples that are similar to the given data.
But if the goal is to obtain samples that sastifying certain conditions, it can be inefficient.
The tilted bootstrap, or bootstrap with tilting [@johns1988], aims to efficiently obtain such samples.



# Model
Denote the observed data as $\{X_1, \ldots, X_n\}$ and assume it comes from a certain distribution family with parameter $\theta=(\theta_1^T, \theta_2^T)^T$. Assume the maximual likelihood estimate of $\hat{\theta}_1$ can be expressed as linear combination of $X_i$ for $i=1,\ldots,n$. This includes many parameters such as mean and variance. 
 
The goal is to obtain samples such that $\hat{\theta}_1 = \theta_{0;1}$.

Assume there is an inclusion probability 
$\boldsymbol{q}=(q_1,\ldots,q_n)^T$ affiliated to the observed data, such that any samples drawn from the observed data with inclusion probability $\boldsymbol{q}$ satisfies 
$$
\hat{\theta}_1^{(b)} = \theta_{0;1}.
$$

There may be multiple $\boldsymbol{q}$ meeting this requirement and different $\boldsymbol{q}$ results into different distribtuion.
One intuitive way is minimizing the KL distance between tilted data and the observed data
$$
\sum_i q_i log\{q_i/(1/n)\}.
$$
The resulted $q_i$ will be proportional to $\exp\{\boldsymbol{X}_i \boldsymbol{\lambda}\}$, which is equivalent to exponential tilting.


# Simulations

## Simulation 1: exponential family density
We simulate 1000 data points from Gaussian distribution $N(\mu, 1)$ for $\mu$ equals 0.25,0.5,0.75 and 1.5 respectively and align their means at 0.

To see the resulted distribution after tilting, we draw 100 bootstrap samples from the generated data points with probability equal to the tilted weight and then run kernel density estimation on each of the bootstrap samples.
The nonparametric kernel density of the tilted distribution is given in parallel with the true density in the plot below. 

```{r}
library(tboot)

library(bindata)
```

```{r, fig.cap="The blue dashed line is the density generating the data;  The black transparent lines are the kernel density on the tilted bootstrap samples (repeated 100 times); the yellow line is the standard normal density). The mean of the true density are 0.25, 0.5, 0.75, 1.5 respectively. This figure shows the tilted bootstrap samples match the target distribution very closely.", fig.align="center", fig.width=6, warning=FALSE}

set.seed(2018)
def.par <- par(no.readonly = TRUE) # save default, for resetting...
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
par(mai = c(.45,.45,.25,.25))
for(bias in c(0.25, .5, 0.75, 1.5)){
  X = matrix(rnorm(1000, bias))
  w = tweights(X, target = 0, silent=T)
  
  bw = max(bw.ucv(X), bw.bcv(X), bw.SJ(X))
  
  curve(dnorm(x), from = -4, to = 4, col="yellow", lwd=2, ylim=c(0,0.48), ylab="density")
  curve(dnorm(x,mean = bias), add=T, col = "blue", lty=2)
  #legend("topright", lwd=2, col="red", legend = "True density")
  for(b in 1:100){
    Xb = tboot(X, weights = w)
    den = density(Xb, bw=bw)
    lines(den, col=rgb(0,0,0,0.1))
  }

}

par(def.par)

```




## Simulation 2: Gaussian Mixture

The tilted Gaussian mixture is still a Gaussian mixture with new means and new priors.


```{r fig.align="center", fig.cap="The generating density is Gaussian mixture 0.4N(mu-2, 1)+0.6N(mu+2,1), marked as blue dashed lines. The kernel density of the  bootstrap samples are marked as black lines. The four figures correspond to mu=0.25, 0.5, 0.75 and 1.5 respectively.", fig.width=6, warning=FALSE}
set.seed(2018)
def.par <- par(no.readonly = TRUE) # save default, for resetting...
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
par(mai = c(.45,.45,.25,.25))
for(bias in c(0.25, .5, 0.75, 2.2)){
  cand1 = rnorm(400, bias-2)
  cand2 = rnorm(600, bias+2)
  X = matrix( ifelse(runif(1000)< 0.4, cand1, cand2) )
  w = tweights(X, target = 0, silent=T)
  
  bw = max(bw.ucv(X), bw.bcv(X), bw.SJ(X))
  
  curve(0.4 * dnorm(x, mean=bias-2)+ 0.6*dnorm(x, mean=bias+2), from = -5, to = 6, col="blue", lty=2, ylim=c(0,0.32), ylab="density")
  #curve(dnorm(x,mean = bias), add=T, col = "blue", lty=2)
  #legend("topright", lwd=2, col="red", legend = "True density")
  for(b in 1:100){
    Xb = tboot(X, weights = w)
    den = density(Xb, bw=bw)
    lines(den, col=rgb(0,0,0,0.1))
  }

}

par(def.par)
```

Generate 1000 data points from $0.4N(\mu-2, 1)+0.6N(\mu+2,1)$ and align the mean to be 0. Repeat the simulation for 1000 times for $\mu$ equals 0.25, 0.5, 0.75 and 1.5 respectively.
The above plots shows the kernel density plot of the tilted distribution.



## Simulation 3: Multivariate Gaussian with some of the means being fixed

In a multivariate Gaussian distribution, if aligning the means of $\boldsymbol{X}_1$ at certain values, the expectation of means of $\boldsymbol{X}_2$ are equal to the conditional mean 
$$
E\{\boldsymbol{X}_2 \mid \boldsymbol{X}_1=\boldsymbol{x}_1\}
$$
while the covariance remains unchanged.




```{r fig.width=6, fig.align="center", fig.cap="Aligning the means of first 3 dimension of a multivariate Gaussian distribution at 0.5. Each boxplot corresponds to the difference of the sample mean from its expectation. The results are based on 1000 simulations."}
set.seed(2018)
n = 1000
mu = runif(6)*2
Sig = diag(6)
rho = -0.6
for(i in 1:6){
  for(j in 1:6){
    Sig[i, j ] = rho^(abs(i-j) )
  }
}

mu_cond = (mu[4:6] + Sig[4:6,1:3] %*% solve(Sig[1:3,1:3]) %*% (0.5-mu[1:3]))
S_cond = Sig[4:6, 4:6] - Sig[4:6,1:3] %*% solve(Sig[1:3,1:3]) %*% Sig[1:3, 4:6]
res = matrix(0, 1000, 6)
S_res = matrix(0, 1000, 36)

for(i in 1:1000){
  X = MASS::mvrnorm(n = n, mu = mu, Sigma = Sig)
  Sig_X = cov(X)
  w = tweights(X[,1:3], target = rep(0.5, 3), silent=T)
  
  res[i,] = crossprod(X[,1:3], w) - 0.5
  res[i,4:6] = crossprod(X[,4:6], w) - (colMeans(X)[4:6] + Sig_X[4:6,1:3] %*% solve(Sig_X[1:3,1:3]) %*% (0.5-colMeans(X)[1:3]))
  S_res[i,] = c(cov.wt(X, w)$cov - Sig)
  
}
boxplot(res)

```

To show this, we generate data from a multivariate Gaussian distribution with means `r round(mu,3)` and covariance matrix $\Sigma$ where $\Sigma_{ij}=(-0.6)^{|i-j|}$, $1\le i,j\le6$.
Aligning the $X_1$, $X_2$ and $X_3$ at 0.5, we expect the sample means of $X_4$, $X_5$ and $X_6$ to be distributed around the conditional means.

By substracting the corresponded expectations from $X_1$, $X_2$, $X_3$, $X_4$, $X_5$ and $X_6$ and repeating the experiments for 1000 times, we obtain the boxplots of all the 6 dimensions.
Clearly, $\frac{1}{n}\sum_{j}X_{ij}=0.5$ for $i=1,2,3$ and 
$$
E\{\frac{1}{n}\sum_{j}X_{ij} \}= E\{X_i | (X_1, X_2, X_3)^T=(0.5,0.5,0.5)^T\}
$$
for $i=4,5,6$.


The variance matrix of the tilted distribution will be exactly the same as the original data.
The following boxplots show the differences between $\tilde{\Sigma}_{ij}$ and $\widehat{\Sigma}_{ij}$ for $1\le i,j\le n$, where $\tilde{\Sigma}_{ij}$ is the element of $\tilde{\Sigma}$, the sample covariance of the tilted distribution while $\widehat{\Sigma}_{ij}$ is the element of $\widehat{\Sigma}$, the sample covariance of original data.


```{r, fig.align="center", fig.cap="The differece between covariance of tilted distribution and the sample covariance.",fig.width=6}
boxplot(S_res, xlab="",xaxt='n', ann=FALSE)
```


## Simulation 4: Multivariate Bernoulli distribution


```{r fig.width=6,fig.align="center"}
set.seed(2018)
n = 1000
mu = runif(6)/2+0.25
Sig = diag(6)
rho = -0.6
for(i in 1:6){
  for(j in 1:6){
    Sig[i, j ] = rho^(abs(i-j) )
  }
}

ind = c(7, 13,14, 19,20,21, 25,26,27,28, 31,32,33,34,35)
Sig_res = matrix(0,100, 36)
res = matrix(0, 100, 6)
for(i in 1:100){
  X = rmvbin(n = n, margprob = mu, sigma = Sig)
  w = tweights(X[,1:3], target = rep(0.5, 3), silent=T)
  res[i,] = crossprod(X, w)
  Sig_res[i,] = c(cov.wt(X, w)$cov - cov(X))
}
Sig_res = as.data.frame(Sig_res)
boxplot(res)

```


The last example is a multivariate Bernoulli distribution.
The means are `r round(mu,3)` while the covariance matrix is $\Sigma$ where $\Sigma_{ij}=(-0.6)^{|i-j|}$.
In the example, the means of the first 3 dimensions are aligned at 0.5.

It is much harder to give a closed form for the conditional means of a multivariate bernoulli distribution but the sample means of $X_4$, $X_5$ and $X_6$ after tilting could serve a good way to approximate them. 

# References


